##### 2009-8-1  ####
(10:33:06 PM) Benjamin Liu: 细小文件将会是个头疼的问题
(10:33:19 PM) Benjamin Liu: 细小文件总共有多少?
(10:34:19 PM) wy'MSN: 细小文件的多少和source file有关
(10:34:50 PM) Benjamin Liu: 假设source有1M
(10:34:59 PM) Benjamin Liu: 那么小文件将有多少呢?
(10:35:46 PM) Benjamin Liu: 去做下统计,然后我们讨论存储方式
(10:36:01 PM) wy'MSN: 我这没有很好的测试文件
(10:36:19 PM) Benjamin Liu: 没有关系
(10:36:39 PM) Benjamin Liu: 只要数量级正确就可以
(10:37:47 PM) Benjamin Liu: 膨胀比率多少?
(10:38:17 PM) wy'MSN: 我暂时还没测很大的文件数
(10:38:29 PM) wy'MSN: 因为还没去掉单个文件
(10:38:35 PM) wy'MSN: 所以出来肯定超级多
(10:38:38 PM) Benjamin Liu: 没有关系,你1M总测过吧
(10:38:54 PM) wy'MSN: ok，我看下
(10:39:37 PM) Benjamin Liu: http://bbs.chinaunix.net/archiver/?tid-616589.html
(10:48:12 PM) wy'MSN: 1M
(10:48:18 PM) wy'MSN: 生成了500K
(10:48:43 PM) Benjamin Liu: 那这样,你不构建自己的index格式的话,你没有别的选择
(10:49:06 PM) Benjamin Liu: 去学习/使用tokyo cabinet,否则你的文件系统会崩溃的.
(10:49:25 PM) Benjamin Liu: tc是个数据库
(10:49:26 PM) wy'MSN: 看了下大部分文件都是要去掉的
(10:49:36 PM) Benjamin Liu: ??什么意思
(10:50:12 PM) wy'MSN: 大部分sub-metadata文件(子问题)只含有一个文件
(10:50:33 PM) wy'MSN: 也就是说这些文件不和任何文件相似
(10:50:44 PM) wy'MSN: 应该要去掉的，只是我还没做
(10:51:07 PM) Benjamin Liu: 他们会存在在文件系统中么?
(10:51:18 PM) wy'MSN: 如果做了，冗余还是很大的话我试试你的方法
(10:51:22 PM) wy'MSN: 不会
(10:51:32 PM) wy'MSN: 他们根本不会生成
(10:51:36 PM) Benjamin Liu: 他们曾经存在么?
(10:51:48 PM) wy'MSN: 在内存中就咔嚓了
(10:51:56 PM) Benjamin Liu: 你的内存能放多少?
(10:52:20 PM) Benjamin Liu: 决定delete的前提是所有小文件都已生成
(10:52:43 PM) Benjamin Liu: 所以你必定需要把它放在外存上,内存放不下
(10:53:50 PM) wy'MSN: 没阿，在扫描排好序的metadata文件，对于没有其他相同md5的md5，那一行我是直接跳过的
(10:54:40 PM) wy'MSN: 这样就会少一些
(10:54:44 PM) Benjamin Liu: 小文件放的是什么?片断的md5
(10:55:21 PM) wy'MSN: 你说的小文件就是 子问题的metadata吧？
(10:55:37 PM) wy'MSN: 里面存的是md5+文件名
(10:56:01 PM) Benjamin Liu: 1个source file,平均对应多少个?
(10:59:36 PM) Benjamin Liu: ?
(11:01:31 PM) wy'MSN: 37个
(11:01:55 PM) Benjamin Liu: 如果你有百万文档,那么将会造成灾难
(11:03:02 PM) wy'MSN: 看了下，我生成的metadata大小是600K
(11:03:48 PM) Benjamin Liu: 让数据库处理这个问题,用tokyo cabinet,这是一个key-value的数据库系统,非常快,应该比文件系统快很多,而且可扩展性好
(11:04:16 PM) Benjamin Liu: 你可以去google一下这个数据库系统,接口很简单
(11:05:44 PM) Benjamin Liu: 你可以自动生成10万个小文件在一个目录里面,你就知道文件大小对文件系统不是太大的问题,文件个数才是灾难
(11:06:11 PM) wy'MSN: 恩，我现在就要做进去？ 我觉得暂时先第一个版本做好吧
(11:06:22 PM) wy'MSN: 恩，这个有体验的
(11:06:28 PM) Benjamin Liu: 这个版本肯定要被咔嚓的
(11:06:48 PM) Benjamin Liu: 明显会抛弃的东西别费功夫了
(11:07:30 PM) Benjamin Liu: python的接口叫做pytc
(11:07:51 PM) wy'MSN: ok
(11:08:29 PM) wy'MSN: 艾，之前的功夫确实基本都白做了阿....
(11:09:04 PM) Benjamin Liu: 就那么300行,叹什么气
(11:09:57 PM) Benjamin Liu: 我估计最后你的总行数在2000-5000,你现在还早呢
(11:10:02 PM) wy'MSN: 但也确实花了不少时间
(11:10:07 PM) wy'MSN: 恩


##### 2009-8-2  ####
(03:26:24 PM) wy'MSN: 今天看了下，发现用全用数据库存的话复杂性会很大，然后又看了下文档，发现他们给出了他们的实验结果：他们是全用metadata文件来保存的，最后的结果也是可以忍受的
(03:26:56 PM) wy'MSN: 不过可以把文件名放在数据库里
(03:27:09 PM) wy'MSN: 这样最初的metadata会小很多
(03:27:47 PM) wy'MSN: 还有 可以加入bin-packing这一步，进一步减少小文件
(03:27:48 PM) Benjamin Liu: 难度什么地方比较大
(03:28:27 PM) wy'MSN: 我要对存的metadata排序
(03:29:18 PM) Benjamin Liu: tokyo cabinet有没有类似于数据库的index或者sorting? 支持你traverse?
(03:30:04 PM) Benjamin Liu: 用文件存储难道你打算在内出里面排序?
(03:31:17 PM) wy'MSN: 没，直接文件内容排序阿
(03:31:26 PM) wy'MSN: 一句sort就解决了
(03:31:36 PM) wy'MSN: 速度相当快
(03:31:36 PM) Benjamin Liu: ???
(03:31:50 PM) Benjamin Liu: 是哪个sort api
(03:32:12 PM) wy'MSN: linux自带的命令
(03:32:22 PM) Benjamin Liu: sort文件名?
(03:32:24 PM) wy'MSN: 可能是shell命令
(03:32:32 PM) Benjamin Liu: sort文件名?
(03:32:37 PM) wy'MSN: sort metadata
(03:32:46 PM) wy'MSN: 这样就好了
(03:32:51 PM) wy'MSN: 我暂时是这么做的
(03:33:47 PM) wy'MSN: 我觉得比在数据库中对几百万条记录排序可能要快...
(03:33:58 PM) Benjamin Liu: 这块一定要改掉
(03:34:23 PM) wy'MSN: why? 实验结果显示是可以的...
(03:34:39 PM) Benjamin Liu: 你可以试一下,500M,你去sort一下,看看性能和结果
(03:35:04 PM) wy'MSN: 文档上也是用sort命令的
(03:35:05 PM) Benjamin Liu: sort用来对常规文件进行处理
(03:36:30 PM) wy'MSN: 目前来看，还是没有要用二进制文件的必要
(03:36:49 PM) wy'MSN: 而且我觉得这个系统对时间要求不高
(03:37:05 PM) wy'MSN: 这个系统应该是很久才运行一次的系统
(03:38:33 PM) wy'MSN: 用数据库来存储文件名的话已经能减少差不多50％的metadata文件大小了
(03:40:12 PM) Benjamin Liu: 1 你的假设是非real time,但是事实不是这样的,它是个实时的增量的
(03:42:55 PM) Benjamin Liu: 2 sort的实现如果只是考虑用户普通文件, 那么在数据量大的时候必定会大量使用内存,内存用完了就用swap,在swap用尽的时候崩溃. 所以你如果不清楚sort的具体实现,或者知道它没有用内外存sorting的方式,那么文章上的假设也是错误的
(03:44:45 PM) Benjamin Liu: 因为sorting, union find都是时间复杂度和空间复杂度的最大的部分,这些地方含糊不得,实时性/可扩展性都有赖于此
(03:48:30 PM) wy'MSN: 实时增量是怎样的形式？ 是告诉我那些文件是新增加的?
(03:48:42 PM) Benjamin Liu: 对
(03:49:08 PM) Benjamin Liu: 作为一个简单的测试,你写一个脚本,将你已经生成的那个文件复制10000遍,变成500M大小的文件,然后你去sort,看看机器是不是会死掉,速度怎么样. 我没看过sort的实现,但我相信它肯定是一个内存排序的算法,binutils不可能发神经去实现外存排序
(03:50:17 PM) wy'MSN: 那我这个稍加修改就可以实现吧？ 给一个接口，把新增加的MD5加到Metadata文件末尾就行了
(03:50:41 PM) wy'MSN: 据文档说sort是disk-based的，具体我再查一下或试一下
(03:51:10 PM) Benjamin Liu: 没有这么简单!你需要插入排序
(03:51:26 PM) Benjamin Liu: 文档让我看一下, sort
(03:52:16 PM) wy'MSN: 我的意思是hp那篇文档
(03:52:25 PM) wy'MSN: 要不我试一下好了
(03:52:32 PM) Benjamin Liu: 你把相应的那句话copy给我看一下
(03:52:50 PM) Benjamin Liu: 要真是这样,我还真白玩linux这么多年了
(03:53:19 PM) wy'MSN: Perform a disk-based sort on this data, as is commonly 
available in UNIX ‘sort’ implementations
(04:03:34 PM) Benjamin Liu: 是我低估了linux/unix,是外排序
(04:04:31 PM) wy'MSN: 恩，很强大的....用的时候也感觉到了
(04:04:36 PM) Benjamin Liu: 用这个命令可以看到它打开了十几个外排桶临时文件 strace sort -S 1K diff_summary 2>/tmp/lll
(04:05:07 PM) Benjamin Liu: open("/tmp/sortI91UeJ", O_RDWR|O_CREAT|O_EXCL|O_LARGEFILE, 0600) = 4  
(04:05:30 PM) wy'MSN: 噢，是用strace追踪的啊？
(04:05:37 PM) wy'MSN: 很强大
(04:08:41 PM) Benjamin Liu: 那么1. 就用sort,不用自己再做多路归并排序了
(04:08:49 PM) Benjamin Liu: 2. 为了保证系统的内存使用的稳定性, 请指定-S参数明确sort可用内存大小,以免出现swap的情况更加牺牲性能. 
(04:08:51 PM) Benjamin Liu: 3. 增量情况暂时可以考虑,用一个大文件和一个小文件放在一起归并排序用sort -m 但是你的那个大文件可能日积月累可能非常的大,需要考虑永久解决方法.
(04:09:29 PM) Benjamin Liu: 有可能出现一个文件还放不下的情况,这时候你怎么办?还得找数据库帮忙,先想好
(04:11:15 PM) Benjamin Liu: 或者自定义格式的metafile.
(04:12:21 PM) Benjamin Liu: 关于碎片文件,一个guideline,不论怎么样不允许一个目录下多于1K个文件存在
(04:12:43 PM) Benjamin Liu: 其他都可以商量
(04:14:21 PM) wy'MSN: ok，这个现做下来看，估计问题不大.若在极端情况下有问题还是可以优化的
(04:14:53 PM) Benjamin Liu: 小文件的个数不是1:37 source file么?真的没有问题?
(04:15:15 PM) wy'MSN: 这个有很多前提
(04:15:52 PM) Benjamin Liu: 那么我们说1:5,公平么?
(04:15:56 PM) wy'MSN: 首先没有去掉单位文件，这些我估计就有至少百分之七八十
(04:16:30 PM) Benjamin Liu: 即使1:1呢?  你认为source file来自于文件系统么?
(04:20:10 PM) wy'MSN: 这个暂时我没法确定，但是我觉得是问题不大。还有如果文件数很大(如超过1K)真的会访问起来很慢吗？这只是我们图形界面下的感觉，用文字界面，甚至直接用程序访问文件系统，这样问题应该不大
(04:20:19 PM) wy'MSN: 当然，要测试过才知道
(04:20:42 PM) Benjamin Liu: 我在intel作linux kernel,你不用测试了
(04:21:00 PM) Benjamin Liu: source file也来自于数据库, 或者自定义格式的index, cache, pipe. 等等, 没有任何工业应用会去忍受上万个文件的文件系统访问的.
(04:23:50 PM) Benjamin Liu: >1K的方案不被接受
(04:24:02 PM) Benjamin Liu: 可以作为过渡阶段测试用
(04:24:07 PM) Benjamin Liu: 这是底线
(04:24:08 PM) wy'MSN: sourcefile 也来自数据库？超多文件的话 这个会比文件系统快很多？
(04:24:12 PM) wy'MSN: 恩，ok
(04:28:25 PM) Benjamin Liu: 对于通用search engine情况来说, source file来自于爬虫, 几百个爬虫把东西抓下来放到一个 几个queue里面, 然后几百个indexer从这几个queue中取出source file, 进行index; 对于小型应用来说source file无一例外都在数据库中,是数据库中的某一个包含纯文本的field. 对于个人应用来说, 比如desktop search可以是五花八门的文件.
(04:30:15 PM) wy'MSN: 恩，好的～ 

==========2009 8.4==========
(08/03/2009 04:34:09 PM) Benjamin Liu: 转贴的威力!
(08/03/2009 04:34:11 PM) Benjamin Liu: http://www.kaixin001.com/!repaste/detail.php?uid=4319028&urpid=308959152#cmt
(08/03/2009 04:34:36 PM) Benjamin Liu: 我马上就能看到闵行交大的游泳池了
(05:26:13 PM) wy'MSN: 恩....这几天交大很壮观....
(05:26:34 PM) wy'MSN: 转贴 类似校内的分享吧。。
(05:27:06 PM) Benjamin Liu: 分享? 你在校内的id高诉我
(05:31:19 PM) wy'MSN: 实名的
(05:31:26 PM) wy'MSN: 就是我的名字
(05:32:05 PM) Benjamin Liu: 加你了,不过我用的是假名
(05:33:13 PM) wy'MSN: http://xiaonei.com/profile.do?id=223881603
(05:33:20 PM) wy'MSN: 我没收到消息
(05:34:02 PM) wy'MSN: 收到了
(05:34:37 PM) Benjamin Liu: 和你说的分享差不多
(05:34:48 PM) Benjamin Liu: 这类应用的鼻祖是www.digg.com
(05:35:07 PM) Benjamin Liu: 你一屏全是分享啊
(05:35:10 PM) wy'MSN: 恩
(05:35:32 PM) wy'MSN: 这个朋友分享了，觉得有趣，我就也分享了
(05:36:18 PM) wy'MSN: digg现在是不是有人工编辑了，不单纯是聚合了
(05:38:23 PM) Benjamin Liu: 应该是你搞错了,他们前100最有影响力的digger可以一定程度上决定上榜和排名
(05:39:10 PM) Benjamin Liu: 你说的应得应该是techmeme.com
(05:40:49 PM) wy'MSN: 恩，查了下是的
(05:42:00 PM) Benjamin Liu: 真正的民主集中制
(05:42:52 PM) wy'MSN: 不过就目前来看，引入用户关系后的聚合才更有活力
(05:43:46 PM) Benjamin Liu: facebook为什么不行呢
(05:44:23 PM) wy'MSN: facebook不行？ 我没这个意思阿
(05:44:41 PM) Benjamin Liu: facebook的转贴为什么不火呢
(05:46:17 PM) wy'MSN: 额，火不火不清楚。至少校内现在是铺天盖地的....要说他们的区别么，传统的digg有分类功能，用户可以选择自己感兴趣的方面，但是SNS中的转贴就比较乱
(05:47:07 PM) wy'MSN: 但是SNS中的用户本群身是一个很好的分类，要是两者结合就更好了
(05:52:08 PM) Benjamin Liu: 思路还是比较清楚的
(05:52:50 PM) Benjamin Liu: facebook上转贴不火是因为已经有digg.com了. 国内仿效facebook的如xiaonei/kaixin001,能把这个应用做火是因为没有国内的digg.com这样影响力的站点.当然现如今简单的copy并不管用
(06:12:00 PM) wy'MSN: 恩，确实国内没发现，只有不怎么纯粹是digg类型的cnbeta还蛮火.不过我感觉国内的人分享精神貌似不怎么高，这可能也是个因素

==========2009 8.10==========
(12:14:44 AM) Benjamin Liu: union find作好了么？
(12:16:16 AM) wy'MSN: 恩，好了～ 现在已经能看到结果了
(12:16:29 AM) Benjamin Liu: 哈哈！
(12:16:33 AM) Benjamin Liu: 好啊！
(12:16:37 AM) Benjamin Liu: 结果如何？
(12:16:44 AM) wy'MSN: 但要优化一下
(12:16:51 AM) Benjamin Liu: 不说速度
(12:16:55 AM) Benjamin Liu: 精度
(12:19:41 AM) wy'MSN: 我没有很好的测试文档，首先测试精度，我复制了十篇左右的文章，分别放在3个地方，都能找出来；然后测试scalability，测试了25M左右的文件夹，能够找出十几个相似文件cluster
(12:21:46 AM) Benjamin Liu: 如果你能找到一些作文数据的话，你就能分析抄袭了
(12:21:51 AM) Benjamin Liu: 赫赫
(12:21:57 AM) wy'MSN: 恩，呵呵～
(12:22:31 AM) Benjamin Liu: 下面就是编写白盒测试用例
(12:23:04 AM) Benjamin Liu: 你在理解逻辑的前提下，能够知道哪里是弱点，哪里是corner case.
(12:23:27 AM) wy'MSN: 25M的那个速度也还行，两分钟左右，最高占用内存是5M左右，
(12:23:36 AM) wy'MSN: 恩
(12:24:07 AM) wy'MSN: 暂时我还要加个步骤
(12:24:22 AM) wy'MSN: 减少中间生成的小文件的数目
(12:24:23 AM) Benjamin Liu: 作为一个可以用的系统，你这25M测试离系统测试还远，
(12:25:19 AM) Benjamin Liu: 系统测试是黑盒，至少500M，然后看性能/瓶颈/scalability，看兼容性
(12:25:35 AM) Benjamin Liu: 小文件绝对要避免
(12:26:40 AM) wy'MSN: 那个步骤加上去后小文件就几乎没有了，测试我没有相关经验，这几天看一下
(12:27:07 AM) wy'MSN: 还有500M的数据也是问题，到哪里去弄呢？
(12:27:54 AM) Benjamin Liu: 你说为啥有数据库？除了支持sql之外，还有避免文件系统的干扰
(12:28:12 AM) Benjamin Liu: 找李老师啊，她有9G数据
(12:28:16 AM) Benjamin Liu: 拿来用啊
(12:29:04 AM) Benjamin Liu: sougou lab，你也可以自己去找http://www.sogou.com/labs/dl/t.html  
(12:29:25 AM) Benjamin Liu: 最关键的是她已经帮你html-->plain text作好了
(12:30:53 AM) wy'MSN: 恩，好的～文件系统的干扰指的是？
(12:49:32 AM) Benjamin Liu: 你知道怎么打开一个文件的对吧，os课上有讲，先要找到其目录，然后找到其文件，其实目录就可以考虑成文件，它的内容就是各个文件的位置，属性，名称等等，如果文件过多，首先打开文件的时候的寻找就需要很多时间基本上就是O( n )
(12:50:48 AM) Benjamin Liu: 这只是其一，你可以问你们操作系统老师还有其他很多和buffer/cache相关的影响
(01:21:24 AM) wy'MSN: 恩，了解了

============  2009 8.12 ===========
(04:27:53 PM) wy'MSN: 李老师说她那边的数据都在研究生那边，要开学才有
(04:28:13 PM) Benjamin Liu: 昏倒
(04:28:48 PM) Benjamin Liu: 你现在小文件问题解决了么?
(04:31:14 PM) wy'MSN: 恩，我现在的方案是 我可以设定我最终的子问题的metadata文件的大小，真正跑起来设为可用内存小些就好了，所以假设我设为100M，那么10个文件就可以处理1G的metadata数据量了，所以小文件现在不是问题了
(04:32:44 PM) Benjamin Liu: 分成十个桶?每个桶里放些碎片?
(04:33:39 PM) wy'MSN: 恩，其实他做sub-metadata文件就是因为可能内存放不下把
(04:34:15 PM) Benjamin Liu: 当然放不下了
(04:34:15 PM) wy'MSN: 如果我一次就可以把所有metadata读进来，那就不用再分成子问题了
(04:34:39 PM) Benjamin Liu: 这个假设必定是错的
(04:34:54 PM) wy'MSN: 恩，意思就是每个子问题尽量让所用内存最大化
(04:34:55 PM) Benjamin Liu: 为何要全读近来? 因为要clustering?
(04:35:00 PM) wy'MSN: 恩
(04:35:38 PM) Benjamin Liu: ... 为什么有10个大文件,而不是20个,不是11个?
(04:36:27 PM) wy'MSN: 这个参数可以自己设置的，就是我每个文件让他放多少子问题
(04:36:59 PM) wy'MSN: 其实我是在生成自问题之后，弄了个简单的装包算法
(04:37:01 PM) Benjamin Liu: 这样的设计会从那些方面slow down整个系统?
(04:37:35 PM) Benjamin Liu: 搜索引擎也有类似的barrel,但是它的算法异常复杂,高效
(04:37:53 PM) Benjamin Liu: 我只是想知道你的现有算法有多山寨
(04:39:27 PM) Benjamin Liu: 你对子问题需要如何retrieve? 需要如何store?
(04:39:34 PM) wy'MSN: 总得来说，就是生成子问题后，以子问题的大小为元素，预先设置的期望的文件大小作为包的大小，进行装包算法
(04:40:30 PM) Benjamin Liu: 子文体就是一组 <md5key-->docid>的pair是么?
(04:41:26 PM) wy'MSN: 恩，对的
(04:41:39 PM) wy'MSN: 就是经过unifind算法得到的
(04:43:01 PM) Benjamin Liu: 你需要怎么再去访问这些东西呢?以clusterid为索引? retrieval决定了storage.
(04:44:05 PM) wy'MSN: 对的，我把clusterid保存下来了，然后生成装箱策略，即每个cluster对应一个子问题号
(04:44:23 PM) wy'MSN: 这个几乎不怎么耗时间的
(04:44:50 PM) wy'MSN: 只不过遍历一下所有cluster号
(04:45:06 PM) Benjamin Liu: 你怎么retrieve这个cluster呢?
(04:46:03 PM) Benjamin Liu: 一个子文体的所有内容需要在内存中么?
(04:46:16 PM) wy'MSN: 在这个算法里我只需要clusterID和他的大小，所以保存在一个map里就好了
(04:46:32 PM) wy'MSN: 是的
(04:47:21 PM) Benjamin Liu: 这样做,cluster怎么生成的呢?
(04:47:44 PM) Benjamin Liu: 内存放不下了?赶出去到硬盘
(04:47:49 PM) Benjamin Liu: 然后merge?
(04:48:09 PM) wy'MSN: 内存肯定放的下
(04:48:21 PM) wy'MSN: 这是我做子问题处理的原因
(04:48:35 PM) wy'MSN: 子问题保证都能放在内存
(04:48:55 PM) Benjamin Liu: 一个子问题一个子问题地处理是么?
(04:49:01 PM) wy'MSN: 对的
(04:49:09 PM) wy'MSN: 目前是的
(04:49:41 PM) wy'MSN: 你说的并行处理我没接触过，以后也许可以家
(04:49:46 PM) Benjamin Liu: 好,写得模式很清楚了,只要输入排好序的md5-->file的文件就行了对巴
(04:50:07 PM) wy'MSN: 对的
(04:50:12 PM) Benjamin Liu: 那么读取的模式呢?你存下来了是为了以后读
(04:50:14 PM) wy'MSN: 这个我自己生成的呀
(04:50:29 PM) Benjamin Liu: 你怎么读?
(04:50:37 PM) wy'MSN: 一行行读
(04:50:43 PM) wy'MSN: 他的格式是不变的
(04:51:13 PM) wy'MSN: sort是按行排序的
(04:52:00 PM) Benjamin Liu: 你这样,你把现有实现总结一下
(04:52:31 PM) Benjamin Liu: 尤其是数据格式,数据流的设计
(04:52:47 PM) Benjamin Liu: 然后发给我,我们打电话讨论一下
(04:52:57 PM) wy'MSN: ok
(04:53:14 PM) Benjamin Liu: 今天能发给我么?
(04:54:12 PM) wy'MSN: 我待会吃完饭回来写吧，写得完的话我发给你，因为我和同学晚上约好了有点事
(04:55:26 PM) Benjamin Liu: 那明天中午或者下午,我们打个电话
(04:56:05 PM) Benjamin Liu: 另外,纯文本语料,你先用http://www.sogou.com/labs/dl/c.html
(04:56:27 PM) wy'MSN: 好....
(04:57:24 PM) Benjamin Liu: 可能不大合适,但是最快的就是它了,下载之后用iconv -f 'gbk' -t 'utf-8' -c 弄成utf-8格式统一处理,碰到编码问题找我,不希望你在编码问题上浪费时间.
(04:57:46 PM) wy'MSN: 恩，行
(04:58:00 PM) wy'MSN: 还有我对白盒、黑盒测试完全没经验，网上看了一圈都好抽象，有什么书或者文章推荐的吗？
(04:58:34 PM) Benjamin Liu: 哈哈,我们明天一起说吧
(04:58:52 PM) wy'MSN: ok～
(04:59:09 PM) Benjamin Liu: 测试只有一些理念可以写成书,真正的实战都和实践相结合的
(04:59:31 PM) Benjamin Liu: 白盒/unit-test很大程度上受开发者的影响
(04:59:53 PM) wy'MSN: 恩
(05:04:01 PM) Benjamin Liu: 你把这个脚本放到你的机器上处理gbk --> utf-8的转化问题
(05:04:31 PM) Benjamin Liu: 这个是perl

=======================2009 8.18 =======================
Benjamin Liu 说:
 测得怎么样了？
龑 说:
 刚把数据库和取出相同文档加进去了，还没怎么测试，但粗略感觉对于相同文档不是很多的效果不是很明显
Benjamin Liu 说:
 接下来就需要找到一个golden parameter了，有几个parameter可调节？
龑 说:
 四五个吧，今天调了下，影响还是蛮大的
Benjamin Liu 说:
 哈哈哈
 可以想象你的测试过程是很山寨的
龑 说:
 主要是文件大 文件小 要分别调参数
 恩...
 明天要给李老师回报一下，我先睡啦~
Benjamin Liu 说:
 这个牛人是做svm的
龑 说:
 嗯？
Benjamin Liu 说:
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
龑 说:
 SVM感觉很难
Benjamin Liu 说:
 第一张图是登高线
 在二维情况下用来发现golden parameter的
龑 说:
 嗯..但是这个通用吗？ 还是它通过测一大堆数据 画出这张图..
Benjamin Liu 说:
 它的整个过程是自动化的
 而且是可视的
 我是说参数搜索
 有点类似于人工神经网络里面的参数搜索了
 你明天找时间琢磨琢磨这里面的意思吧。反正它有一个假设--就是参数和精度共同构成的空间肯定是连续的
龑 说:
 ok
Benjamin Liu 说:
 猜对了
龑 说:
 我确实是要在速度和精度之间做权衡
 明天我看下..
Benjamin Liu 说:
 1 对大部分应用可行，你给我找个离散的问题试试看
 2 他是通过多次测试得到的
龑 说:
 哦，对SVM还是没概念。。明天研究下
 
 ===================== 2009 8.19 ====================
 2009-8-19  12:47:46  Benjamin Liu  龑  最近没联系李老师,她怎么说?  
2009-8-19  12:51:47  龑  Benjamin Liu  他说实验室人都走光了，要开学才能测数据，她让我在自己机器上测试 
2009-8-19  12:55:12  龑  Benjamin Liu  在我机器上测得话 如果要测几百兆的东西不知道要测多久了，我可以试一下 
2009-8-19  12:55:48  Benjamin Liu  龑  你原来的多久? 
2009-8-19  12:56:05  Benjamin Liu  龑  平均一M数据有多久? 
2009-8-19  12:56:25  Benjamin Liu  龑  她对你的进展应该挺满意吧 
2009-8-19  12:56:30  龑  Benjamin Liu  恩 
2009-8-19  12:57:46  Benjamin Liu  龑  你原来的多久? 平均一M数据有多久?  
2009-8-19  12:58:15  龑  Benjamin Liu  这个和我要求的精度有关，如果我把阈值设为0.6（百分之六十相同），那么平均1M 4、5秒左右吧 
2009-8-19  12:58:43  龑  Benjamin Liu  不过我只测了搜狗的那个 
2009-8-19  12:58:55  龑  Benjamin Liu  时间和预料的特点很有关系 
2009-8-19  12:59:16  Benjamin Liu  龑  一天有86400second 
2009-8-19  13:00:04  龑  Benjamin Liu  恩，但时间肯定不是随文件大小线性增长的 
2009-8-19  13:00:24  Benjamin Liu  龑  照这样算,试试吧 
2009-8-19  13:00:57  龑  Benjamin Liu  那一天就有21G左右吧 
2009-8-19  13:01:38  Benjamin Liu  龑  呵呵,一个台阶一个台阶地试验吧 
2009-8-19  13:01:54  Benjamin Liu  龑  0.5hour, 1hour, 2 hour, 4 hour, 8 hour 
2009-8-19  13:02:40  龑  Benjamin Liu  额，好吧...我的机器要受苦了 
2009-8-19  13:03:12  Benjamin Liu  龑  哈哈,这样的机器等淘汰的时候你会觉得用得值了. 
2009-8-19  13:04:02  龑  Benjamin Liu  呵呵 :) 
2009-8-19  13:04:26  Benjamin Liu  龑  你毕业论文要有所突破的话,可以试图去写文档去重的参数自动化搜索,这个领域我估计能评上优秀论文. 
2009-8-19  13:05:57  Benjamin Liu  龑  当然了,你也知道,那就是paper work了 
2009-8-19  13:06:14  龑  Benjamin Liu  恩，参数的调整很弹性，也很难调 速度和精度之间无法同时保证，要权衡 而这两者合输入文件的平均大小，文件的特征(重复文档是否多)都有关 
2009-8-19  13:06:34  龑  Benjamin Liu  嗯，真要找出它们的关系还需要做很多事情 
2009-8-19  13:06:48  Benjamin Liu  龑  计算机的论文, 很多牛paper都是数学很牛的人 
2009-8-19  13:06:59  Benjamin Liu  龑  你数学分析多少分? 
2009-8-19  13:07:09  龑  Benjamin Liu  80+ 
2009-8-19  13:07:13  龑  Benjamin Liu  具体忘了。。 
2009-8-19  13:07:17  Benjamin Liu  龑  还不错啊,小伙子 
2009-8-19  13:07:30  Benjamin Liu  龑  我那时候最高到过89 
2009-8-19  13:07:54  龑  Benjamin Liu  额，估计难度是不一样的吧。。反正我觉得我的数学基础不咋地 
2009-8-19  13:08:04  龑  Benjamin Liu  后悔没学学好啊 
2009-8-19  13:08:07  Benjamin Liu  龑  要有信心 
2009-8-19  13:08:19  Benjamin Liu  龑  爱因斯坦还是个数学白吃 
2009-8-19  13:08:28  龑  Benjamin Liu  啊，真的啊 
2009-8-19  13:08:37  Benjamin Liu  龑  你去查 
2009-8-19  13:08:53  Benjamin Liu  龑  他助手帮助他分析他的直觉 
2009-8-19  13:09:08  龑  Benjamin Liu  呵呵，囧 
2009-8-19  13:09:17  Benjamin Liu  龑  :) 
2009-8-19  13:09:46  Benjamin Liu  龑  你能做出来今天的这个东西不是也就靠这一股劲么? 要有信心, 信心是自己给的 
2009-8-19  13:10:31  龑  Benjamin Liu  恩，对的~:D 
2009-8-19  13:10:51  龑  Benjamin Liu  进了大学才知道数学很重要，那是高考结束我还在想进了大学死也不碰数学了 
2009-8-19  13:11:59  Benjamin Liu  龑  哈哈,不要把它当你的敌人,它是你很有用的工具,等你觉得它有用了,就是你的好伙伴了 
2009-8-19  13:12:23  Benjamin Liu  龑  不过爱因斯坦告诉我一个真理--科学基本靠猜 
2009-8-19  13:12:34  龑  Benjamin Liu  呵呵 
2009-8-19  13:12:42  Benjamin Liu  龑  哈哈,直觉是基础,数学分析是一个工具 
2009-8-19  13:12:55  龑  Benjamin Liu  要大胆猜想，小心论证 
2009-8-19  13:15:41  Benjamin Liu  龑  呵呵,小伙子悟性很好啊 
2009-8-19  13:18:46  龑  Benjamin Liu  :) 学生和在外混了很久的人想法的角度、广度和深度很不一样，经历很重要 
2009-8-19  13:19:16  龑  Benjamin Liu  不过在学校的好处就是可以不受打扰地学自己感兴趣的东西 
2009-8-19  13:20:12  Benjamin Liu  龑  没错 
2009-8-19  13:20:43  Benjamin Liu  龑  等你这个项目告一段落, 我们花时间好好聊聊 
2009-8-19  13:21:36  龑  Benjamin Liu  ok~ 这三个月估计会是我进大学以来最忙的三个月了 


==========  2009 8.22   =========================
2009-8-22  21:14:32  Benjamin Liu  龑  现在参数调节得怎么样了? 
2009-8-22  22:42:26  龑  Benjamin Liu  参数的话对结果影响较大的的主要是阈值，但是在阈值接近1时对结果的影响也就不大了，所以暂时主要测试是放在对不同大小的输入语料进行时间测试 
2009-8-22  22:46:25  龑  Benjamin Liu  测下来 一个是机器性能确实挺重要，同样的输入，在实验室机器跑的时间是在我机器上的一半 另一个是现在算法时间主要是花在最后一步上，而且确实是接近n平方， 总时间的话现在测试100M的东西，在实验室机器上是20分钟样子，我的机器上要近40分钟 
2009-8-22  22:48:14  龑  Benjamin Liu  今天把实验室的机器环境配置好了，明天去做更多的测试，主要还是时间的上的测试，暂时想到一个方法也许能发最后一步的时间复杂度降到nlogn，明天试一下 
2009-8-22  23:03:19  Benjamin Liu  龑  非常好! n*logn这是很理想的结果 
2009-8-22  23:03:40  Benjamin Liu  龑  李老师同意给你的是什么配置的机器? 
2009-8-22  23:04:45  龑  Benjamin Liu  具体没看，在台机中性能应该也一般吧，我明天留意一下 
2009-8-22  23:04:52  龑  Benjamin Liu  是一个研究生用的 
2009-8-22  23:05:12  Benjamin Liu  龑  哈哈,已经把你当研究生了 
2009-8-22  23:05:57  龑  Benjamin Liu  呵呵，不过她那里机器确实少。。几台还是旧的 
2009-8-22  23:06:15  龑  Benjamin Liu  开机嘎嘎直响的 
2009-8-22  23:09:35  Benjamin Liu  龑  如果n*logn实验失败或者不符合你的样例集合特点,你可以看看knuth的书找思路, 
2009-8-22  23:13:12  龑  Benjamin Liu  恩，好的~ 李老师让我帮她把实验室里的的语料库去一下重，结果今天研究生拷给我一个目录，里面有十万个文件，在console下复制直接超出cp命令的参数数目限制，而且超慢 
2009-8-22  23:13:33  Benjamin Liu  龑  接下来大的方面就几件事情: 0.参数调节,准确比快更重要,而且这个参数配置看看是否具有普遍的适应性--如果不是,再想办法. 1. 为了这个正确性测试必须想办法做一套正确性测试用例出来,不管有100篇还是10000篇. 2. 性能:关键部分的瓶颈优化n*logn 3.可扩展性如果你已经考虑了内存不够的情况就基本上可以了. 4. 大数据量测试,验证之前的各种假设, 可以找到很多功能/性能的bug. 5. 找找方法进行参数自适应调节--或者李老师能给你什么更好的题目做去写论文. 
2009-8-22  23:14:03  Benjamin Liu  龑  哈哈哈,李老师终于提供预料了 
2009-8-22  23:14:03  Benjamin Liu  龑  语料 
2009-8-22  23:14:22  Benjamin Liu  龑  html语料?还是text语料? 
2009-8-22  23:14:29  龑  Benjamin Liu  text 
2009-8-22  23:14:43  Benjamin Liu  龑  great! 
2009-8-22  23:15:04  Benjamin Liu  龑  你让研究生给你配个hadoop 
2009-8-22  23:15:12  龑  Benjamin Liu  嗯？ 干嘛的？ 
2009-8-22  23:15:22  Benjamin Liu  龑  问google 
2009-8-22  23:15:59  龑  Benjamin Liu  ok 
2009-8-22  23:18:50  Benjamin Liu  龑  那个有点远了,但是心里有根弦就行了--distributed computing/storage 
2009-8-22  23:23:02  龑  Benjamin Liu  好~ 现在企业级的分布式存储业都是用Hadoop吗？ 
2009-8-22  23:29:31  Benjamin Liu  龑  分布式有很多方案,hadoop是个开源的,而且yahoo支持,所以应用的比较广 
2009-8-22  23:36:32  龑  Benjamin Liu  恩，好 
